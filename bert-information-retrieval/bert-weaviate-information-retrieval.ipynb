{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install tensorflow\n",
    "# !pip3 install torch\n",
    "# !pip3 install weaviate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the BERT transformer model and pytorch\n",
    "\n",
    "We are using the `bert-base-uncased` model in this example, but any model will work. Feel free to adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# udpated to use different model if desired\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "\n",
    "# Create model and tokenizer\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weaviate Client\n",
    "This assumes you have Weaviate running locally on `:8080`. Adjust URL accordingly. You could also enter the WCS URL here, for example, if you are running a WCS cloud instance instead of running Weaviate locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.Client(\"http://localhost:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset from disk\n",
    "Create some helper functions to create the dataset (20-newsgroup text posts) from disk. These methods are specific to the structure of your dataset, adjust accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def get_post_filenames(limit_objects=100):\n",
    "    file_names = []\n",
    "    i=0\n",
    "    for root, dirs, files in os.walk(\"./data/20news-bydate-test\"):\n",
    "        for filename in files:\n",
    "            path = os.path.join(root, filename)\n",
    "            file_names += [path]\n",
    "        \n",
    "    random.shuffle(file_names)\n",
    "    limit_objects = min(len(file_names), limit_objects)\n",
    "      \n",
    "    file_names = file_names[:limit_objects]\n",
    "\n",
    "    return file_names\n",
    "\n",
    "def read_posts(filenames=[]):\n",
    "    posts = []\n",
    "    for filename in filenames:\n",
    "        f = open(filename, encoding=\"utf-8\", errors='ignore')\n",
    "        ## TODO: strip headers\n",
    "        post = f.read()\n",
    "        \n",
    "        # strip the headers (the first occurrence of two newlines)\n",
    "        post = post[post.find('\\n\\n'):]\n",
    "        \n",
    "        # remove posts with less than 10 words to remove some of the noise\n",
    "        if len(post.split(' ')) < 10:\n",
    "               continue\n",
    "        \n",
    "        post = post.replace('\\n', ' ').replace('\\t', ' ')\n",
    "        if len(post) > 1000:\n",
    "            post = post[:1000]\n",
    "        posts += [post]\n",
    "\n",
    "    return posts       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize Dataset using BERT\n",
    "\n",
    "The following is a helper function to vectorize all posts (using our BERT transformer) which are entered as an array. The return array contains all the vectors in the same order. BERT is optimized to run on GPUs, if you're using CPUs this might take a while. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def vectorize_posts(posts=[]):\n",
    "    print(\"Vectorize your posts with BERT. If you are using CPUs this might take a while...\")\n",
    "    post_vectors=[]\n",
    "    before=time.time()\n",
    "    for i, post in enumerate(posts):\n",
    "        sentences = sent_tokenize(post)\n",
    "        tokens_pt = tokenizer(sentences, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=\"pt\")\n",
    "        outputs = model(**tokens_pt)\n",
    "        vec = outputs[0].mean(0).mean(0).detach()\n",
    "        post_vectors += [vec]\n",
    "        if i % 25 == 0 and i != 0:\n",
    "            print(\"So far {} objects vectorized in {}s\".format(i, time.time()-before))\n",
    "    after=time.time()\n",
    "    \n",
    "    print(\"Vectorized {} items in {}s\".format(len(posts), after-before))\n",
    "    \n",
    "    return post_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run everything we have so far\n",
    "\n",
    "It is now time to run the functions we defined before. Let's load 50 random posts from disk, then vectorize them using BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize your posts with BERT. If you are using CPUs this might take a while...\n",
      "So far 25 objects vectorized in 17.35766339302063s\n",
      "So far 50 objects vectorized in 30.970993280410767s\n",
      "So far 75 objects vectorized in 48.50780916213989s\n",
      "So far 100 objects vectorized in 58.79331040382385s\n",
      "So far 125 objects vectorized in 74.6122612953186s\n",
      "So far 150 objects vectorized in 89.20110416412354s\n",
      "So far 175 objects vectorized in 101.08796119689941s\n",
      "So far 200 objects vectorized in 112.2345552444458s\n",
      "So far 225 objects vectorized in 128.05660319328308s\n",
      "So far 250 objects vectorized in 144.34292316436768s\n",
      "So far 275 objects vectorized in 159.30127835273743s\n",
      "So far 300 objects vectorized in 175.7815442085266s\n",
      "So far 325 objects vectorized in 190.5825710296631s\n",
      "So far 350 objects vectorized in 206.8574640750885s\n",
      "So far 375 objects vectorized in 220.1920862197876s\n",
      "So far 400 objects vectorized in 233.265465259552s\n",
      "So far 425 objects vectorized in 246.8308732509613s\n",
      "So far 450 objects vectorized in 264.2033841609955s\n",
      "So far 475 objects vectorized in 277.7190201282501s\n",
      "So far 500 objects vectorized in 289.08016204833984s\n",
      "So far 525 objects vectorized in 303.0833442211151s\n",
      "So far 550 objects vectorized in 314.8939950466156s\n",
      "So far 575 objects vectorized in 327.0519323348999s\n",
      "So far 600 objects vectorized in 345.08833408355713s\n",
      "So far 625 objects vectorized in 362.84299516677856s\n"
     ]
    }
   ],
   "source": [
    "posts = read_posts(get_post_filenames(4000))\n",
    "vectors = vectorize_posts(posts)\n",
    "print(len(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weaviate\n",
    "\n",
    "Now that we have vectors we can import both the posts and the vectors into Weaviate, so we can then search through them.\n",
    "\n",
    "### Init a simple schema\n",
    "Our schema is very simple, we just have one object class, the \"Post\". A post class has just a single property, which we call \"content\" and is of type \"text\".\n",
    "\n",
    "Each class in schema creates one index, so by running the below we tell weaviate to create one brand new vector index waiting for us to import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weaviate_schema(client):\n",
    "    \n",
    "    # a simple schema containing just a single class for our posts\n",
    "    schema = {\n",
    "        \"classes\": [{\n",
    "                \"class\": \"Post\",\n",
    "                \"vectorizer\": \"none\", # explicitly tell Weaviate not to vectorize anything, we are providing the vectors ourselves through our BERT model\n",
    "                \"properties\": [{\n",
    "                    \"name\": \"content\",\n",
    "                    \"dataType\": [\"text\"],\n",
    "                }]\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    # cleanup from previous runs\n",
    "    client.schema.delete_all()\n",
    "    \n",
    "    client.schema.create(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weaviate_schema(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## doing this manually until the client is updated\n",
    "import requests\n",
    "\n",
    "def import_posts_with_vectors(posts, vectors):\n",
    "    if len(posts) != len(vectors):\n",
    "        raise Exception(\"len of posts ({}) and vectors ({}) does not match\".format(len(posts), len(vectors)))\n",
    "        \n",
    "    for i, post in enumerate(posts):\n",
    "        r = requests.post('http://localhost:8080/v1/objects', json={\n",
    "            \"class\": \"Post\",\n",
    "            \"vector\": vectors[i].tolist(),\n",
    "            \"properties\": {\n",
    "                \"content\": post,\n",
    "            }\n",
    "        })\n",
    "             \n",
    "        if r.status_code > 399:\n",
    "            print(res)\n",
    "        \n",
    "    \n",
    "import_posts_with_vectors(posts, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query \"camera lenses\" with 3 results took 0.014058113098144531s\n",
      "---\n",
      "0.8743142:    Nikon L35 Af camera. 35/2.8 lens and camera case. Package $50  Send e-mail \n",
      "---\n",
      "0.7892951:        I have a few 12\" composite monochrome monitors for sale.  Magnovax Computer Monitor 80, Model number BM7650 074B.  RCA type input for video only. (no audio).  Power, Brightness and Contrast dials in front, V and H hold and position controls on the back. Nice little monitor that can be used for  PCs, Amigas, your VCR, security monitor.  Excellent condition. I am asking for $40 plus shipping and COD (not to exceed $10) if applicable.      \n",
      "---\n",
      "0.7800093:   my 14\" compacq vga monitor id dead due to the transformer's failure.  if you have this part and would like to get rid of it, pls let me know.  thanks.  eric \n"
     ]
    }
   ],
   "source": [
    "# search query \n",
    "query = \"camera lenses\"\n",
    "limit = 3\n",
    "\n",
    "def query_to_vector(query=\"\"):\n",
    "    tokens_pt = tokenizer(query, padding=True, truncation=True, max_length=500, add_special_tokens = True, return_tensors=\"pt\")\n",
    "    outputs = model(**tokens_pt)\n",
    "    return outputs[0].mean(0).mean(0).detach()\n",
    "\n",
    "search_vec = {\"vector\": query_to_vector(query).tolist()}\n",
    "\n",
    "before = time.time()\n",
    "res = client \\\n",
    "    .query.get(\"Post\", [\"content\", \"_additional {certainty}\"]) \\\n",
    "    .with_near_vector(search_vec) \\\n",
    "    .with_limit(limit) \\\n",
    "    .do()\n",
    "after = time.time()\n",
    "\n",
    "print(\"Query \\\"{}\\\" with {} results took {}s\".format(query, limit, after-before))\n",
    "for post in res[\"data\"][\"Get\"][\"Post\"]:\n",
    "    print('---')\n",
    "    print(\"{}: {}\".format(post[\"_additional\"][\"certainty\"], post[\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
